<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Configuration settings · LogDevice</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## Admin API/server"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Configuration settings · LogDevice"/><meta property="og:type" content="website"/><meta property="og:url" content="https://logdevice.io/index.html"/><meta property="og:description" content="## Admin API/server"/><meta property="og:image" content="https://logdevice.io/img/logdevice_og.png"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://logdevice.io/img/logdevice.png"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://logdevice.io/blog/atom.xml" title="LogDevice Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://logdevice.io/blog/feed.xml" title="LogDevice Blog RSS Feed"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><link rel="stylesheet" href="/css/main.css"/></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/facebook_logdevice_whitewordmark.png" alt="LogDevice"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/docs/Overview.html" target="_self">Docs</a></li><li class=""><a href="/api/annotated.html" target="_self">API</a></li><li class=""><a href="/help.html" target="_self">Support</a></li><li class=""><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Configuration</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Getting started</h3><ul><li class="navListItem"><a class="navItem" href="/docs/Overview.html">Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/Installation.html">Installation</a></li><li class="navListItem"><a class="navItem" href="/docs/LocalCluster.html">Running a local cluster</a></li><li class="navListItem"><a class="navItem" href="/docs/Concepts.html">Concepts and architecture</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Configuration</h3><ul><li class="navListItem"><a class="navItem" href="/docs/FirstCluster.html">Creating your first cluster</a></li><li class="navListItem"><a class="navItem" href="/docs/Config.html">Cluster config</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/Settings.html">Settings</a></li><li class="navListItem"><a class="navItem" href="/docs/Logs.html">Logs</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Administration</h3><ul><li class="navListItem"><a class="navItem" href="/docs/LDShell.html">LogDevice Shell</a></li><li class="navListItem"><a class="navItem" href="/docs/LDQuery.html">LDQuery</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">API</h3><ul><li class="navListItem"><a class="navItem" href="/docs/API_Intro.html">Introduction</a></li><li class="navListItem"><a class="navItem" href="/docs/API_Doxygen.html">Reference</a></li></ul></div></div></section></div><script>
            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              const headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                if (event.target.tagName === 'A') {
                  document.body.classList.remove('tocActive');
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docMainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle">Configuration settings</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="admin-api-server"></a><a href="#admin-api-server" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Admin API/server</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>safety-check-max-logs-in-flight</td><td>The number of concurrent logs that we runs checks against during execution of the CheckImpact operation either internally during a maintenance or through the Admin API's checkImpact() call</td><td style="text-align:center">1000</td><td>server only</td></tr>
<tr><td>safety-check-timeout</td><td>The total time the safety check should take to run. This is the time that the CheckImpact operation need to take to scan all logs along with all the historical metadata to ensure than a maintenance is safe</td><td style="text-align:center">10min</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="batching-and-compression"></a><a href="#batching-and-compression" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batching and compression</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>buffered-writer-bg-thread-bytes-threshold</td><td>BufferedWriter can send batches to a background thread.  For small batches, where the overhead dominates, this will just slow things down.  If the total size of the batch is less than this, it will constructed / compressed on the Worker thread, blocking other appends to all logs in that shard.  If larger, it will be enqueued to a helper thread.</td><td style="text-align:center">4096</td><td></td></tr>
<tr><td>buffered-writer-zstd-level</td><td>Zstd compression level to use in BufferedWriter.</td><td style="text-align:center">1</td><td></td></tr>
<tr><td>sequencer-batching</td><td>Accumulate appends from clients and batch them together to create fewer records in the system</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>sequencer-batching-compression</td><td>Compression setting for sequencer batching (if used).</td><td style="text-align:center">zstd</td><td>requires restart, server only</td></tr>
<tr><td>sequencer-batching-passthru-threshold</td><td>Sequencer batching (if used) will pass through any appends with payload size over this threshold (if positive).  This saves us a compression round trip when a large batch comes in from BufferedWriter and the benefit of batching and recompressing would be small.</td><td style="text-align:center">-1</td><td>server only</td></tr>
<tr><td>sequencer-batching-size-trigger</td><td>Sequencer batching (if used) flushes buffered appends for a log when the total amount of buffered uncompressed data reaches this many bytes (if positive).</td><td style="text-align:center">-1</td><td>requires restart, server only</td></tr>
<tr><td>sequencer-batching-time-trigger</td><td>Sequencer batching (if used) flushes buffered appends for a log when the oldest buffered append is this old.</td><td style="text-align:center">1s</td><td>requires restart, server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="configuration"></a><a href="#configuration" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Configuration</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>check-metadata-log-empty-timeout</td><td>Timeout for request that verifies that a metadata log does not already exist for a log that is presumed new and whose metadata provisioning has been initiated by a sequencer activation</td><td style="text-align:center">300s</td><td>server only</td></tr>
<tr><td>client-config-fetch-allowed</td><td>If true, servers will be allowed to fetch configs from the client side of a connection during config synchronization.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>config-path</td><td>location of the cluster config file to use. Format: [file:]&lt;path-to-config-file&gt; or configerator:&lt;configerator-path&gt;</td><td style="text-align:center"></td><td>CLI only, requires restart, server only</td></tr>
<tr><td>enable-logsconfig-manager</td><td>If true, logdeviced will load the logs configuration from the internal replicated storage and will ignore the logs section in the config file. This also enables the remote management API for logs config.</td><td style="text-align:center">true</td><td></td></tr>
<tr><td>file-config-update-interval</td><td>interval at which to poll config file for changes (if reading config from file on disk</td><td style="text-align:center">10000ms</td><td>CLI only</td></tr>
<tr><td>initial-config-load-timeout</td><td>maximum time to wait for initial server configuration until giving up</td><td style="text-align:center">15s</td><td>CLI only, requires restart, server only</td></tr>
<tr><td>logsconfig-manager-grace-period</td><td>Grace period before making a change to the logs config available to the server.</td><td style="text-align:center">0ms</td><td></td></tr>
<tr><td>logsconfig-max-delta-bytes</td><td>How many bytes of deltas to keep in the logsconfig deltas log before we snapshot it.</td><td style="text-align:center">10485760</td><td>server only</td></tr>
<tr><td>logsconfig-max-delta-records</td><td>How many delta records to keep in the logsconfig deltas log before we snapshot it.</td><td style="text-align:center">4000</td><td>server only</td></tr>
<tr><td>logsconfig-snapshotting-period</td><td>Controls time based snapshotting. New logsconfig snapshot will be created after this period if there are new log configuration deltas</td><td style="text-align:center">1h</td><td>server only</td></tr>
<tr><td>max-sequencer-background-activations-in-flight</td><td>Max number of concurrent background sequencer activations to run. Background sequencer activations perform log metadata changes (reprovisioning) when the configuration attributes of a log change.</td><td style="text-align:center">20</td><td>server only</td></tr>
<tr><td>on-demand-logs-config</td><td>Set this to true if you want the client to get log configuration on demand from the server when log configuration is not included in the main config file.</td><td style="text-align:center">false</td><td>requires restart, client only</td></tr>
<tr><td>on-demand-logs-config-retry-delay</td><td>When a client's attempt to get log configuration information from server on demand fails, the client waits this much before retrying.</td><td style="text-align:center">5ms..1s</td><td>client only</td></tr>
<tr><td>remote-logs-config-cache-ttl</td><td>The TTL for cache entries for the remote logs config. If the logs config is not available locally and is fetched from the server, this will determine how fresh the log configuration used by the client will be.</td><td style="text-align:center">60s</td><td>requires restart, client only</td></tr>
<tr><td>sequencer-background-activation-retry-interval</td><td>Retry interval on failures (or retries due to running the queue for too long while processing background sequencer activations for reprovisioning.</td><td style="text-align:center">10ms</td><td>server only</td></tr>
<tr><td>sequencer-epoch-store-write-retry-delay</td><td>The retry delay for sequencer writing log metadata into the epoch store during log reconfiguration.</td><td style="text-align:center">5s..1min-2x</td><td>server only</td></tr>
<tr><td>sequencer-historical-metadata-retry-delay</td><td>The retry delay for sequencer reading metadata log for historical epoch metadata during log reconfiguration.</td><td style="text-align:center">5s..1min-2x</td><td>server only</td></tr>
<tr><td>sequencer-metadata-log-write-retry-delay</td><td>The retry delay for sequencer writing into its own metadata log during log reconfiguration.</td><td style="text-align:center">500ms..30s-2x</td><td>server only</td></tr>
<tr><td>zk-config-polling-interval</td><td>polling and retry interval for Zookeeper config source</td><td style="text-align:center">1000ms</td><td>CLI only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="core-settings"></a><a href="#core-settings" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Core settings</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>admin-enabled</td><td>Is Admin API enabled?</td><td style="text-align:center">true</td><td>requires restart, <strong>experimental</strong>, server only</td></tr>
<tr><td>append-timeout</td><td>Timeout for appends. If omitted the client timeout will be used.</td><td style="text-align:center"></td><td>client only</td></tr>
<tr><td>command-port</td><td>TCP port on which the server listens to for admin commands, supports commands over SSL</td><td style="text-align:center">5440</td><td>requires restart, server only</td></tr>
<tr><td>findkey-timeout</td><td>Findkey API call timeout. If omitted the client timeout will be used.</td><td style="text-align:center"></td><td>client only</td></tr>
<tr><td>log-file</td><td>write server error log to specified file instead of stderr</td><td style="text-align:center"></td><td>server only</td></tr>
<tr><td>loglevel</td><td>One of the following: critical, error, warning, info, debug</td><td style="text-align:center">info</td><td>server only</td></tr>
<tr><td>logsconfig-timeout</td><td>Timeout for LogsConfig API requests. If omitted the client timeout will be used.</td><td style="text-align:center"></td><td>client only</td></tr>
<tr><td>max-nodes</td><td>maximum number of nodes in the cluster. Used for sizing data structures of the failure detector.</td><td style="text-align:center">512</td><td>requires restart, server only</td></tr>
<tr><td>meta-api-timeout</td><td>Timeout for trims/isLogEmpty/tailLSN/datasize API/etc. If omitted the client timeout will be used.</td><td style="text-align:center"></td><td>client only</td></tr>
<tr><td>my-location</td><td>{client-only setting}. Specifies the location of the machine running the client. Used for determining whether to use SSL based on --ssl-boundary. Also used in local SCD reading. Format: &quot;{region}.{dc}.{cluster}.{row}.{rack}&quot;.</td><td style="text-align:center"></td><td>requires restart, client only</td></tr>
<tr><td>port</td><td>TCP port on which the server listens for non-SSL clients</td><td style="text-align:center">16111</td><td>CLI only, requires restart, server only</td></tr>
<tr><td>server-id</td><td>optional server ID, reported by INFO admin command</td><td style="text-align:center"></td><td>requires restart, server only</td></tr>
<tr><td>shutdown-timeout</td><td>amount of time to wait for the server to shut down before terminating the process. Consider modifying --time-delay-before-force-abort when changing this value.</td><td style="text-align:center">120s</td><td>server only</td></tr>
<tr><td>time-delay-before-force-abort</td><td>Time delay before force abort of remaining work is attempted during shutdown. The value is in 50ms time periods. The quiescence condition is checked once every 50ms time period. When the timer expires for the first time, all pending requests are aborted and the timer is restarted. On second expiration all remaining TCP connections are reset (RST packets sent).</td><td style="text-align:center">400</td><td>server only</td></tr>
<tr><td>unmap-caches</td><td>unmap RocksDB block cache before dumping core (reduces core file size)</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>user</td><td>user to switch to if server is run as root</td><td style="text-align:center"></td><td>requires restart, server only</td></tr>
<tr><td>zk-create-root-znodes</td><td>If &quot;false&quot;, the root znodes for a tier should be pre-created externally before logdevice can do any ZooKeeper epoch store operations</td><td style="text-align:center">true</td><td><strong>experimental</strong>, server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="failure-detector"></a><a href="#failure-detector" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Failure detector</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>cluster-state-refresh-interval</td><td>how frequently to search for the sequencer in case of an append timeout</td><td style="text-align:center">10s</td><td>client only</td></tr>
<tr><td>enable-initial-get-cluster-state</td><td>Enable executing a GetClusterState request to retrieve the state of the cluster as soon as the client is created</td><td style="text-align:center">true</td><td>client only</td></tr>
<tr><td>failover-blacklist-threshold</td><td>How many gossip intervals to ignore a node for after it performed a graceful failover</td><td style="text-align:center">100</td><td>server only</td></tr>
<tr><td>failover-wait-time</td><td>How long to wait for the failover request to be propagated to other nodes</td><td style="text-align:center">3s</td><td>server only</td></tr>
<tr><td>gcs-wait-duration</td><td>How long to wait for get-cluster-state reply to come, to initialize state of cluster nodes. Bringup is sent after this reply comes or after timeout</td><td style="text-align:center">1s</td><td>server only</td></tr>
<tr><td>gossip-interval</td><td>How often to send a gossip message. Lower values improve detection time, but make nodes more chatty.</td><td style="text-align:center">100ms</td><td>requires restart, server only</td></tr>
<tr><td>gossip-logging-duration</td><td>How long to keep logging FailureDetector/Gossip related activity after server comes up.</td><td style="text-align:center">0s</td><td>server only</td></tr>
<tr><td>gossip-mode</td><td>How to select a node to send a gossip message to. One of: 'round-robin', 'random' (default)</td><td style="text-align:center">random</td><td>requires restart, server only</td></tr>
<tr><td>gossip-threshold</td><td>Specifies after how many gossip intervals of inactivity a node is marked as dead. Lower values reduce detection time, but make false positives more likely.</td><td style="text-align:center">30</td><td>server only</td></tr>
<tr><td>gossip-time-skew</td><td>How much delay is acceptable in receiving a gossip message.</td><td style="text-align:center">10s</td><td>server only</td></tr>
<tr><td>ignore-isolation</td><td>Ignore isolation detection. If set, a sequencer will accept append operations even if a majority of nodes appear to be dead.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>min-gossips-for-stable-state</td><td>After receiving how many gossips, should the FailureDetector consider itself stable and start doing state transitions of cluster nodes based on incoming gossips.</td><td style="text-align:center">3</td><td>server only</td></tr>
<tr><td>suspect-duration</td><td>How long to keep a node in an intermediate state before marking it as available. Larger values make the cluster less prone to node flakiness, but extend the time needed for sequencer nodes to start participating.</td><td style="text-align:center">10s</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="logsdb"></a><a href="#logsdb" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LogsDB</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>manual-compact-interval</td><td>minimal interval between consecutive manual compactions on log storesthat are out of disk space</td><td style="text-align:center">1h</td><td>server only</td></tr>
<tr><td>rocksdb-background-wal-sync</td><td>Perform all RocksDB WAL syncs on a background thread rather than synchronously on a 'fast' storage thread executing the write.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>rocksdb-directory-consistency-check-period</td><td>LogsDB will compare all on-disk directory entries with the in-memory directory no more frequently than once per this period of time.</td><td style="text-align:center">5min</td><td>server only</td></tr>
<tr><td>rocksdb-free-disk-space-threshold-low</td><td>Keep free disk space above this fraction of disk size by marking node full if we exceed it, and let the sequencer initiate space-based retention. Only counts logdevice data, so storing other data on the disk could cause it to fill up even with space-based retention enabled. 0 means disabled.</td><td style="text-align:center">0</td><td>server only</td></tr>
<tr><td>rocksdb-metadata-compaction-period</td><td>Metadata column family will be compacted at least this often if it has more than one sst file. This is needed to avoid performance issues in rare cases. Full scenario: suppose all writes to this node stopped; eventually all logs will be fully trimmed, and logsdb directory will be emptied by deleting each key; these deletes will usually be flushed in sst files different than the ones where the original entries are; this makes iterator operations very expensive because merging iterator has to skip all these deleted entries in linear time; this is especially bad for findTime. If we compact every hour, this badness would last for at most an hour.</td><td style="text-align:center">1h</td><td>server only</td></tr>
<tr><td>rocksdb-new-partition-timestamp-margin</td><td>Newly created partitions will get starting timestamp <code>now + new\_partition\_timestamp\_margin</code>. This absorbs the latency of creating partition and possible small clock skew between sequencer and storage node. If creating partition takes longer than that, or clock skew is greater than that, FindTime may be inaccurate. For reference, as of August 2017, creating a partition typically takes ~200-800ms on HDD with ~1100 existing partitions.</td><td style="text-align:center">10s</td><td>server only</td></tr>
<tr><td>rocksdb-num-metadata-locks</td><td>number of lock stripes to use to perform LogsDB metadata updates</td><td style="text-align:center">256</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-partition-compaction-schedule</td><td>If set, indicate that the node wil run compaction. This is a list of durations indicating at what age to compact partition.  e.g. &quot;3d, 7d&quot; means that each partition will be compacted twice: when all logs with backlog of up to 3 days are trimmed from it, and when all logs with backlog of up to 7 days are trimmed from it. &quot;auto&quot; (default) means use all backlog durations from config. &quot;disabled&quot; disables partition compactions.</td><td style="text-align:center">auto</td><td>server only</td></tr>
<tr><td>rocksdb-partition-compactions-enabled</td><td>perform background compactions for space reclamation in LogsDB</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>rocksdb-partition-count-soft-limit</td><td>If the number of partitions in a shard reaches this value, some measures will be taken to limit the creation of new partitions: partition age limit is tripled; partition file limit is ignored; partitions are not pre-created on startup; partitions are not prepended for records with small timestamp. This limit is intended mostly as protection against timestamp outliers: e.g. if we receive a STORE with zero timestamp, without this limit we would create over a million partitions to cover the time range from 1970 to now.</td><td style="text-align:center">2000</td><td>server only</td></tr>
<tr><td>rocksdb-partition-duration</td><td>create a new partition when the latest one becomes this old; 0 means infinity</td><td style="text-align:center">15min</td><td>server only</td></tr>
<tr><td>rocksdb-partition-file-limit</td><td>create a new partition when the number of level-0 files in the existing partition exceeds this threshold; 0 means infinity</td><td style="text-align:center">200</td><td>server only</td></tr>
<tr><td>rocksdb-partition-hi-pri-check-period</td><td>how often a background thread will check if new partition should be created</td><td style="text-align:center">2s</td><td>server only</td></tr>
<tr><td>rocksdb-partition-lo-pri-check-period</td><td>how often a background thread will trim logs and check if old partitions should be dropped or compacted, and do the drops and compactions</td><td style="text-align:center">30s</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-file-num-threshold</td><td>don't consider file ranges for partial compactions (used during rebuilding) that are shorter than this</td><td style="text-align:center">10</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-file-size-threshold</td><td>the largest L0 files that it is beneficial to compact on their own. Note that we can still compact larger files than this if that enables usto compact a longer range of consecutive files.</td><td style="text-align:center">50000000</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-largest-file-share</td><td>Partial compaction candidate file ranges that contain a file that comprises a larger propotion of the total file size in the range than this setting, will not be considered.</td><td style="text-align:center">0.7</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-max-file-size</td><td>the maximum size of an l0 file to consider for compaction. If not set, defaults to 2x --rocksdb-partition-partial-compaction-file-size-threshold</td><td style="text-align:center">0</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-max-files</td><td>the maximum number of files to compact in a single partial compaction</td><td style="text-align:center">100</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-max-num-per-loop</td><td>How many partial compactions to do in a row before re-checking if there are higher priority things to do (like dropping partitions). This value is not important; used for tests.</td><td style="text-align:center">4</td><td>server only</td></tr>
<tr><td>rocksdb-partition-partial-compaction-stall-trigger</td><td>Stall rebuilding writes if partial compactions are outstanding in at least this many partitions. 0 means infinity.</td><td style="text-align:center">50</td><td>server only</td></tr>
<tr><td>rocksdb-partition-redirty-grace-period</td><td>Minumum guaranteed time period for a node to re-dirty a partition after a MemTable is flushed without incurring a syncronous write penalty to update the partition dirty metadata.</td><td style="text-align:center">5s</td><td>server only</td></tr>
<tr><td>rocksdb-partition-size-limit</td><td>create a new partition when size of the latest partition exceeds this threshold; 0 means infinity</td><td style="text-align:center">6G</td><td>server only</td></tr>
<tr><td>rocksdb-partition-timestamp-granularity</td><td>minimum and maximum timestamps of a partition will be updated this often</td><td style="text-align:center">5s</td><td>server only</td></tr>
<tr><td>rocksdb-prepended-partition-min-lifetime</td><td>Avoid dropping newly prepended partitions for this amount of time.</td><td style="text-align:center">300s</td><td>server only</td></tr>
<tr><td>rocksdb-proactive-compaction-enabled</td><td>If set, indicate that we're going to proactively compact all partitions (besides two latest) that were never compacted. Compacting will be done in low priority background thread</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rocksdb-read-find-time-index</td><td>If set to true, the operation findTime will use the findTime index to seek to the LSN instead of doing a binary search in the partition.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rocksdb-read-only</td><td>Open LogsDB in read-only mode</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-sbr-force</td><td>If true, space based retention will be done on the storage side, irrespective of whether sequencer initiated it or not. This is meant to make a node's storage available in case there is a critical bug.</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>rocksdb-track-iterator-versions</td><td>Track iterator versions for the &quot;info iterators&quot; admin command</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rocksdb-unconfigured-log-trimming-grace-period</td><td>A grace period to delay trimming of records that are no longer in the config. The intent is to allow the oncall enough time to restore a backup of the config, in case the log(s) shouldn't have been removed.</td><td style="text-align:center">4d</td><td>server only</td></tr>
<tr><td>rocksdb-use-copyset-index</td><td>If set to true, the read path will use the copyset index to skip records that do not pass copyset filters. This greatly improves the efficiency of reading and rebuilding if records are large (1KB or bigger). For small records, the overhead of maintaining the copyset index negates the savings. <strong>WARNING</strong>: if this setting is enabled, records written without --write-sticky-copysets will be skipped by the copyset filter and will not be delivered to readers. Enable --write-sticky-copysets first and wait for all data records written before --write-sticky-copysets was enabled (if any) to be trimmed before enabling this setting.</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-verify-checksum-during-store</td><td>If true, verify checksum on every store. Reject store on failure and return E::CHECKSUM_MISMATCH.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>rocksdb-worker-blocking-io-threshold</td><td>Log a message if a blocking file deletion takes at least this long on a Worker thread</td><td style="text-align:center">10ms</td><td>server only</td></tr>
<tr><td>sbr-low-watermark-check-interval</td><td>Time after which space based trim check can be done on a nodeset</td><td style="text-align:center">60s</td><td>server only</td></tr>
<tr><td>sbr-node-threshold</td><td>threshold fraction of full nodes which triggers space-based retention, if enabled (sequencer-only option), 0 means disabled</td><td style="text-align:center">0</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="monitoring"></a><a href="#monitoring" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Monitoring</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>client-readers-flow-tracer-period</td><td>Period for logging in logdevice_readers_flow scuba table and for triggering certain sampling actions for monitoring. Set it to 0 to disable feature.</td><td style="text-align:center">0s</td><td>client only</td></tr>
<tr><td>disable-trace-logger</td><td>If disabled, NoopTraceLogger will be used, otherwise FBTraceLogger is used</td><td style="text-align:center">false</td><td>requires restart</td></tr>
<tr><td>message-tracing-log-level</td><td>For messages that pass the message tracing filters, emit a log line at this level. One of: critical, error, warning, notify, info, debug, spew</td><td style="text-align:center">info</td><td></td></tr>
<tr><td>message-tracing-peers</td><td>Emit a log line for each sent/received message to/from the specified address(es). Separate different addresses with a comma, prefix unix socket paths with 'unix://'. An empty unix path will match all unix paths</td><td style="text-align:center"></td><td></td></tr>
<tr><td>message-tracing-types</td><td>Emit a log line for each sent/received message of the type(s) specified. Separate different types with a comma.</td><td style="text-align:center"></td><td></td></tr>
<tr><td>publish-single-histogram-stats</td><td>If true, single histogram values will be published alongside the rate values.</td><td style="text-align:center">false</td><td></td></tr>
<tr><td>reader-lagging-threshold</td><td>Amount of time we wait before we report a read stream that is considered lagging.</td><td style="text-align:center">2min</td><td></td></tr>
<tr><td>reader-stalled-grace-period</td><td>Amount of time we wait before declaring a reader stalled because we can't read the metadata or data log. When this grace period expires, the client stat &quot;read_streams_stalled&quot; is bumped and record to scuba</td><td style="text-align:center">30s</td><td></td></tr>
<tr><td>reader-stuck-threshold</td><td>Amount of time we wait before we report a read stream that is considered stuck.</td><td style="text-align:center">121s</td><td></td></tr>
<tr><td>request-exec-threshold</td><td>Request Execution time beyond which it is considered slow, and 'worker_slow_requests' stat is bumped</td><td style="text-align:center">10ms</td><td></td></tr>
<tr><td>shadow-client-timeout</td><td>Timeout to use for shadow clients. See traffic-shadow-enabled.</td><td style="text-align:center">30s</td><td>client only</td></tr>
<tr><td>slow-background-task-threshold</td><td>Background task execution time beyond which it is considered slow, and we log it</td><td style="text-align:center">100ms</td><td></td></tr>
<tr><td>stats-collection-interval</td><td>How often to collect and submit stats upstream.  Set to &lt;=0 to disable collection of stats.</td><td style="text-align:center">60s</td><td>requires restart</td></tr>
<tr><td>trace-all-db-shards</td><td>enable I/O tracing on all database shards</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>trace-db-shard</td><td>enable I/O tracing on the database shard with this index</td><td style="text-align:center">-1</td><td>requires restart, server only</td></tr>
<tr><td>traffic-shadow-enabled</td><td>Controls the traffic shadowing feature. Defaults to false to disable shadowing on all clients writing to a cluster. Must be set to true to allow traffic shadowing, which will then be controlled on a per-log basic through parameters in LogsConfig.</td><td style="text-align:center">false</td><td>client only</td></tr>
<tr><td>watchdog-abort-on-stall</td><td>Should we abort logdeviced if watchdog detected stalled workers.</td><td style="text-align:center">false</td><td></td></tr>
<tr><td>watchdog-bt-ratelimit</td><td>Maximum allowed rate of printing backtraces.</td><td style="text-align:center">10/120s</td><td>requires restart</td></tr>
<tr><td>watchdog-poll-interval</td><td>Interval after which watchdog detects stuck workers</td><td style="text-align:center">5000ms</td><td>requires restart</td></tr>
<tr><td>watchdog-print-bt-on-stall</td><td>Should we print backtrace of stalled workers.</td><td style="text-align:center">true</td><td></td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="network-communication"></a><a href="#network-communication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Network communication</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>checksumming-blacklisted-messages</td><td>Used to control what messages shouldn't be checksummed at the protocol layer</td><td style="text-align:center"></td><td>requires restart, <strong>experimental</strong></td></tr>
<tr><td>checksumming-enabled</td><td>A switch to turn on/off checksumming for all LogDevice protocol messages. If false: no checksumming is done, If true: checksumming-blacklisted-messages is consulted.</td><td style="text-align:center">false</td><td><strong>experimental</strong></td></tr>
<tr><td>command-conn-limit</td><td>Maximum number of concurrent admin connections</td><td style="text-align:center">32</td><td>server only</td></tr>
<tr><td>connect-throttle</td><td>timeout after it which two nodes retry to connect when they loose a a connection. Used in ConnectThrottle to ensure we don't retry too  often.</td><td style="text-align:center">1ms..10s</td><td></td></tr>
<tr><td>connect-timeout</td><td>connection timeout when establishing a TCP connection to a node</td><td style="text-align:center">100ms</td><td></td></tr>
<tr><td>connect-timeout-retry-multiplier</td><td>Multiplier that is applied to the connect timeout after every failed connection attempt</td><td style="text-align:center">3</td><td></td></tr>
<tr><td>connection-backlog</td><td>(server-only setting) Maximum number of incoming connections that have been accepted by listener (have an open FD) but have not been processed by workers (made logdevice protocol handshake).</td><td style="text-align:center">2000</td><td>server only</td></tr>
<tr><td>connection-retries</td><td>the number of TCP connection retries before giving up</td><td style="text-align:center">4</td><td></td></tr>
<tr><td>handshake-timeout</td><td>LogDevice protocol handshake timeout</td><td style="text-align:center">1s</td><td></td></tr>
<tr><td>include-destination-on-handshake</td><td>Include the destination node ID in the LogDevice protocol handshake. If the actual node ID of the connection target does not match the intended destination ID, the connection is terminated.</td><td style="text-align:center">true</td><td></td></tr>
<tr><td>max-protocol</td><td>maximum version of LogDevice protocol that the server/client will accept</td><td style="text-align:center">85</td><td></td></tr>
<tr><td>nagle</td><td>enable Nagle's algorithm on TCP sockets. Changing this setting on-the-fly will not apply it to existing sockets, only to newly created ones</td><td style="text-align:center">false</td><td></td></tr>
<tr><td>outbuf-kb</td><td>max output buffer size (userspace extension of socket sendbuf) in KB. Changing this setting on-the-fly will not apply it to existing sockets, only to newly created ones</td><td style="text-align:center">32768</td><td></td></tr>
<tr><td>outbytes-mb</td><td>per-thread limit on bytes pending in output evbuffers (in MB)</td><td style="text-align:center">512</td><td></td></tr>
<tr><td>rcvbuf-kb</td><td>TCP socket rcvbuf size in KB. Changing this setting on-the-fly will not apply it to existing sockets, only to newly created ones</td><td style="text-align:center">-1</td><td></td></tr>
<tr><td>read-messages</td><td>read up to this many incoming messages before returning to libevent</td><td style="text-align:center">128</td><td></td></tr>
<tr><td>sendbuf-kb</td><td>TCP socket sendbuf size in KB. Changing this setting on-the-fly will not apply it to existing sockets, only to newly created ones</td><td style="text-align:center">-1</td><td></td></tr>
<tr><td>tcp-keep-alive-intvl</td><td>TCP keepalive interval. The interval between successive probes.If negative the OS default will be used.</td><td style="text-align:center">-1</td><td></td></tr>
<tr><td>tcp-keep-alive-probes</td><td>TCP keepalive probes. How many unacknowledged probes before the connection is considered broken. If negative the OS default will be used.</td><td style="text-align:center">-1</td><td></td></tr>
<tr><td>tcp-keep-alive-time</td><td>TCP keepalive time. This is the time, in seconds, before the first probe will be sent. If negative the OS default will be used.</td><td style="text-align:center">-1</td><td></td></tr>
<tr><td>tcp-user-timeout</td><td>The time in miliseconds that transmitted data may remain unacknowledgedbefore TCP will close the connection. 0 for system default. -1 to disable. default is 5min = 300000</td><td style="text-align:center">300000</td><td></td></tr>
<tr><td>use-tcp-keep-alive</td><td>Enable TCP keepalive for all connections</td><td style="text-align:center">true</td><td></td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="performance"></a><a href="#performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>allow-reads-on-workers</td><td>If false, all rocksdb reads are done from storage threads. If true, a cache-only reading attempt is made from worker thread first, and a storage thread task is scheduled only if the cache wasn't enough to fulfill the read. Disabling this can be used for: working around rocksdb bugs; working around latency spikes caused by cache-only reads being slow sometimes</td><td style="text-align:center">true</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>disable-check-seals</td><td>if true, 'get sequencer state' requests will not be sending 'check seal' requests that they normally do in order to confirm that this sequencer is the most recent one for the log. This saves network and CPU, but may cause getSequencerState() calls to return stale results. Intended for use in production emergencies only.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>findtime-force-approximate</td><td>(server-only setting) Override the client-supplied FindKeyAccuracy with FindKeyAccuracy::APPROXIMATE. This makes the resource requirements of FindKey requests small and predictable, at the expense of accuracy</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>write-find-time-index</td><td>Set this to true if you want findTime index to be written. A findTime index speeds up findTime() requests by maintaining an index from timestamps to LSNs in LogsDB data partitions.</td><td style="text-align:center">false</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="read-path"></a><a href="#read-path" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Read path</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>client-epoch-metadata-cache-size</td><td>maximum number of entries in the client-side epoch metadata cache. Set it to 0 to disable the epoch metadata cache.</td><td style="text-align:center">50000</td><td>requires restart, client only</td></tr>
<tr><td>client-initial-redelivery-delay</td><td>Initial delay to use when reader application rejects a record or gap</td><td style="text-align:center">1s</td><td></td></tr>
<tr><td>client-is-log-empty-grace-period</td><td>After receiving responses to an isLogEmpty() request from an f-majority of nodes, wait up to this long for more nodes to chime in if there is not yet consensus.</td><td style="text-align:center">5s</td><td><strong>experimental</strong>, client only</td></tr>
<tr><td>client-max-redelivery-delay</td><td>Maximum delay to use when reader application rejects a record or gap</td><td style="text-align:center">30s</td><td></td></tr>
<tr><td>client-read-buffer-size</td><td>number of records to buffer per read stream in the client object while reading. If this setting is changed on-the-fly, the change will only apply to new reader instances</td><td style="text-align:center">512</td><td></td></tr>
<tr><td>client-read-flow-control-threshold</td><td>threshold (relative to buffer size) at which the client broadcasts window update messages (less means more often)</td><td style="text-align:center">0.7</td><td></td></tr>
<tr><td>data-log-gap-grace-period</td><td>When non-zero, replaces gap-grace-period for data logs.</td><td style="text-align:center">0ms</td><td></td></tr>
<tr><td>gap-grace-period</td><td>gap detection grace period for all logs, including data logs, metadata logs, and internal state machine logs. Millisecond granularity. Can be 0.</td><td style="text-align:center">100ms</td><td></td></tr>
<tr><td>grace-counter-limit</td><td>Maximum number of consecutive grace periods a storage node may fail to send a record or gap (if in all read all mode) before it is considered disgraced and client read streams no longer wait for it. If all nodes are disgraced or in GAP state, a gap record is issued. May be 0. Set to -1 to disable grace counters and use simpler logic: no disgraced nodes, issue gap record as soon as grace period expires.</td><td style="text-align:center">2</td><td></td></tr>
<tr><td>log-state-recovery-interval</td><td>interval between consecutive attempts by a storage node to obtain the attributes of a log residing on that storage node Such 'log state recovery' is performed independently for each log upon the first request to start delivery of records of that log. The attributes to be recovered include the LSN of the last cumulatively released record in the log, which may have to be requested from the log's sequencer over the network.</td><td style="text-align:center">500ms</td><td>requires restart, server only</td></tr>
<tr><td>max-record-bytes-read-at-once</td><td>amount of RECORD data to read from local log store at once</td><td style="text-align:center">1048576</td><td>server only</td></tr>
<tr><td>metadata-log-gap-grace-period</td><td>When non-zero, replaces gap-grace-period for metadata logs.</td><td style="text-align:center">0ms</td><td></td></tr>
<tr><td>output-max-records-kb</td><td>amount of RECORD data to push to the client at once</td><td style="text-align:center">1024</td><td></td></tr>
<tr><td>reader-reconnect-delay</td><td>When a reader client loses a connection to a storage node, delay after which it tries reconnecting.</td><td style="text-align:center">10ms..30s</td><td>client only</td></tr>
<tr><td>reader-retry-window-delay</td><td>When a reader client fails to send a WINDOW message, delay after which it retries sending it.</td><td style="text-align:center">10ms..30s</td><td>client only</td></tr>
<tr><td>reader-started-timeout</td><td>How long a reader client waits for a STARTED reply from a storage node before sending a new START message.</td><td style="text-align:center">30s..5min</td><td>client only</td></tr>
<tr><td>real-time-eviction-threshold-bytes</td><td>When the real time buffer reaches this size, we evict entries.</td><td style="text-align:center">80000000</td><td>requires restart, <strong>experimental</strong>, server only</td></tr>
<tr><td>real-time-max-bytes</td><td>Max size (in bytes) of released records that we'll keep around to use for real time reads.  Includes some cache overhead, so for small records, you'll store less record data than this.</td><td style="text-align:center">100000000</td><td>requires restart, <strong>experimental</strong>, server only</td></tr>
<tr><td>real-time-reads-enabled</td><td>Turns on the experimental real time reads feature.</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>scd-copyset-reordering-max</td><td>SCDCopysetReordering values that clients may ask servers to use.  Currently available options: none, hash-shuffle (default), hash-shuffle-client-seed. hash-shuffle results in only one storage node reading a record block from disk, and then serving it to multiple readers from the cache. hash-shuffle-client-seed enables multiple storage nodes to participate in reading the log, which can be benefit non-disk-bound workloads.</td><td style="text-align:center">hash-shuffle</td><td></td></tr>
<tr><td>unreleased-record-detector-interval</td><td>Time interval at which to check for unreleased records in storage nodes. Any log which has unreleased records, and for which no records have been released for two consecutive unreleased-record-detector-intervals, is suspected of having a dead sequencer. Set to 0 to disable check.</td><td style="text-align:center">30s</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="reader-failover"></a><a href="#reader-failover" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reader failover</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>reader-slow-shards-detection</td><td>If true, readers in SCD mode will detect shards that are very slow andmay ask the other storage shards to filter them out</td><td style="text-align:center">disabled</td><td>client only</td></tr>
<tr><td>reader-slow-shards-detection-moving-avg-duration</td><td>When slow shards detection is enabled, duration to use for the moving average</td><td style="text-align:center">30min</td><td>client only</td></tr>
<tr><td>reader-slow-shards-detection-outlier-duration</td><td>When slow shards detection is enabled, amount of time that we'll consider a shard an outlier if it is slow.</td><td style="text-align:center">1min..30min</td><td>client only</td></tr>
<tr><td>reader-slow-shards-detection-outlier-duration-decrease-rate</td><td>When slow shards detection is enabled, rate at which we decrease the time after which we'll try to reinstate an outlier in the read set. If the value is 0.25, for each second of healthy reading we will decrease that time by 0.25s.</td><td style="text-align:center">0.25</td><td>client only</td></tr>
<tr><td>reader-slow-shards-detection-required-margin</td><td>When slow shards detection is enabled, sensitivity of the outlier detection algorithm. For instance, if set to 3.0, only consider an outlier a shard that is 300% slower than the others. The required margin is adaptive and may increase or decrease but will be capped at a minimum defined by this setting.</td><td style="text-align:center">10.0</td><td>client only</td></tr>
<tr><td>reader-slow-shards-detection-required-margin-decrease-rate</td><td>Rate at which we decrease the required margin when we are healthy. If the value is 0.25 for instance, we will reduce the required margin by 0.25 for every second spent reading.</td><td style="text-align:center">0.25</td><td>client only</td></tr>
<tr><td>scd-all-send-all-timeout</td><td>Timeout after which ClientReadStream fails over to asking all storage nodes to send everything they have if it is not able to make progress for some time</td><td style="text-align:center">600s</td><td></td></tr>
<tr><td>scd-timeout</td><td>Timeout after which ClientReadStream considers a storage node down if it does not send any data for some time but the socket to it remains open.</td><td style="text-align:center">300s</td><td></td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="rebuilding"></a><a href="#rebuilding" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Rebuilding</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>allow-conditional-rebuilding-restarts</td><td>Used to gate the feature described in T22614431. We want to enable it only after all clients have picked up the corresponding change in RSM protocol.</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>auto-mark-unrecoverable-timeout</td><td>If too many storage nodes or shards are declared down by the failure detector or RocksDB, readers stall. If this setting is 'max' (default), readers remain stalled until some shards come back, or until the shards are marked _unrecoverable_ (permanently lost) by writing a special in the event log via an admin tool. If this setting contains a time value, upon the timer expiration the shards are marked unrecoverable automatically. This allows reader clients to skip over all records that could only be delivered by now unrecoverable shards, and continue reading more recent records.</td><td style="text-align:center">max</td><td>server only</td></tr>
<tr><td>event-log-grace-period</td><td>grace period before considering event log caught up</td><td style="text-align:center">10s</td><td>server only</td></tr>
<tr><td>event-log-max-delta-bytes</td><td>How many bytes of deltas to keep in the event log before we snapshot it.</td><td style="text-align:center">10485760</td><td>server only</td></tr>
<tr><td>event-log-max-delta-records</td><td>How many delta records to keep in the event log before we snapshot it.</td><td style="text-align:center">100</td><td>server only</td></tr>
<tr><td>event-log-retention</td><td>How long to keep a history of snapshots and deltas for the event log. Unused if the event log has never been snapshotted or if event log trimming is disabled with disable-event-log-trimming.</td><td style="text-align:center">14d</td><td>server only</td></tr>
<tr><td>event-log-snapshot-compression</td><td>Use ZSTD compression to compress event log snapshots</td><td style="text-align:center">false</td><td></td></tr>
<tr><td>event-log-snapshotting</td><td>Allow the event log to be snapshotted onto a snapshot log. This requires the event log group to contain two logs, the first one being the snapshot log and the second one being the delta log.</td><td style="text-align:center">true</td><td>requires restart</td></tr>
<tr><td>eventlog-snapshotting-period</td><td>Controls time based snapshotting. New eventlog snapshot will be created after this period if there are new deltas</td><td style="text-align:center">1h</td><td>server only</td></tr>
<tr><td>max-log-rebuilding-size-mb</td><td>Maximum amount of memory that can be consumed by a single LogRebuilding state machine</td><td style="text-align:center">5</td><td>server only</td></tr>
<tr><td>max-node-rebuilding-percentage</td><td>Do not initiate rebuilding if more than this percentage of storage nodes in the cluster appear to have been lost or have shards that appear to require rebuilding.</td><td style="text-align:center">35</td><td>server only</td></tr>
<tr><td>max-rebuilding-trigger-queue-size</td><td>Maximum number of triggers in the rebuilding supervisor queue before it switches to throttling mode. This translates to the acceptable number of dead nodes for which we can trigger rebuilding simultaneously. This setting should allow at least one full rack to be queued in order to support rack failures. A negative value indicates that the rebuilding supervisor should compute a reasonable value for this setting automatically, based on the cluster configuration.</td><td style="text-align:center">-1</td><td>server only</td></tr>
<tr><td>rebuild-dirty-shards</td><td>On start-up automatically rebuild LogsDB partitions left dirty by a prior unsafe shutdown of this node. This is called mini-rebuilding. The setting should be on unless you are running with --append-store-durability=sync_write, or don't care about data loss.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>rebuild-store-durability</td><td>The minimum guaranteed durablity of rebuilding writes before a storage node will confirm the STORE as successful. Can be one of &quot;memory&quot;, &quot;async_write&quot;, or &quot;sync_write&quot;. See --append-store-durability for a description of these options.</td><td style="text-align:center">async_write</td><td>server only</td></tr>
<tr><td>rebuilding-checkpoint-interval-mb</td><td>Write a per-log rebuilding checkpoint once per this many megabytes of rebuilt data in the log. A rebuilding checkpoints contains an LSN through which the log has been rebuilt by this donor and the rebuilding version number identifying this rebuilding run. If a node restarts in the middle of a rebuilding run, it resumes rebuilding of a log from that log's last checkpoint.</td><td style="text-align:center">100</td><td>server only</td></tr>
<tr><td>rebuilding-global-window</td><td>the size of rebuilding global window expressed in units of time. The global rebuilding window is an experimental feature similar to the local window, but tracking rebuilding reads across all storage nodes in the cluster rather than per node. Whereas the local window improves the locality of reads, the global window is expected to improve the locality of rebuilding writes.</td><td style="text-align:center">max</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>rebuilding-local-window</td><td>the size of rebuilding local window expressd in units of time. In the current implementation of rebuilding each log is rebuilt independently. The local window forces all rebuilding reads on a given node to be at most the local window size apart. Reading on logs that are read too fast is stalled until lagging logs catch up. This improves the locality of reading from LogsDB and makes the disk IO pattern more sequential.</td><td style="text-align:center">20min</td><td>server only</td></tr>
<tr><td>rebuilding-local-window-uses-partition-boundary</td><td>If true, the local window will be moved on partition boundaries. If false, it will instead be moved on fixed time intervals, as set by --rebuilding-local-window.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>rebuilding-max-amends-in-flight</td><td>maximum number of requests to update (amend) a rebuilt record's copyset that a rebuilding donor node can have in flight at the same time, per log.</td><td style="text-align:center">100</td><td>server only</td></tr>
<tr><td>rebuilding-max-batch-bytes</td><td>max amount of data that a node can read in one batch for rebuilding</td><td style="text-align:center">10485760</td><td>server only</td></tr>
<tr><td>rebuilding-max-get-seq-state-in-flight</td><td>maximum number of 'get sequencer state' requests that a rebuilding donor node can have in flight at the same time. Every storage node participating in rebuilding gets the sequencer state for all logs residing on that node before beginning to re-replicate records. This is done in order to determine the LSN at which to stop rebuilding the log.</td><td style="text-align:center">100</td><td>server only</td></tr>
<tr><td>rebuilding-max-logs-in-flight</td><td>maximum number of logs that a donor node can be rebuilding at the same time.</td><td style="text-align:center">1</td><td>server only</td></tr>
<tr><td>rebuilding-max-records-in-flight</td><td>maximum number of rebuilding STORE requests that a rebuilding donor node can have in flight at the same time, per log</td><td style="text-align:center">5</td><td>server only</td></tr>
<tr><td>rebuilding-planner-sync-seq-retry-interval</td><td>retry interval for individual 'get sequencer state' requests issued by rebuilding via SyncSequencerRequest API, with exponential backoff</td><td style="text-align:center">60s..5min</td><td>server only</td></tr>
<tr><td>rebuilding-restarts-grace-period</td><td>Grace period used to throttle how often rebuilding can be restarted. This protects the server against a spike of messages in the event log that would cause a restart.</td><td style="text-align:center">20s</td><td>server only</td></tr>
<tr><td>rebuilding-store-timeout</td><td>Maximum timeout for attempts by rebuilding to store a record copy or amend a copyset on a specific storage node. This timeout only applies to stores and amends that appear to be in flight; a smaller timeout (--rebuilding-retry-timeout) is used if something is known to be wrong with the store, e.g. we failed to send the message, or we've got an unsuccessful reply, or connection closed after we sent the store.</td><td style="text-align:center">240s..480s</td><td>server only</td></tr>
<tr><td>rebuilding-use-iterator-cache</td><td>Place rebuilding iterators in the LogsDB iterator cache.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rebuilding-use-rocksdb-cache</td><td>Allow rebuilding reads to use RocksDB block cache.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>rebuilding-wait-purges-backoff-time</td><td>Retry timeout for waiting for local shards to purge a log before rebuilding it.</td><td style="text-align:center">1s..10s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>record-durability-timeout</td><td>Time for which LogRebuilding/RebuidlingCoordinator will wait for pending records to be durable before restarting the rebuilding for the log.</td><td style="text-align:center">960s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>reject-stores-based-on-copyset</td><td>If true, logdevice will prevent writes to nodes that are being drained (rebuilt in RELOCATE mode). Not recommended to set to false unless you're having a production issue.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>self-initiated-rebuilding-grace-period</td><td>grace period in seconds before triggering full node rebuilding after detecting the node has failed.</td><td style="text-align:center">1200s</td><td>server only</td></tr>
<tr><td>total-log-rebuilding-size-per-shard-mb</td><td>Maximum amount of memory that can be consumed by all LogRebuilding state machines, per shard</td><td style="text-align:center">100</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="recovery"></a><a href="#recovery" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Recovery</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>concurrent-log-recoveries</td><td>limit on the number of logs that can be in recovery at the same time</td><td style="text-align:center">400</td><td>server only</td></tr>
<tr><td>enable-record-cache</td><td>Enable caching of unclean records on storage nodes. Used to minimize local log store access during log recovery.</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
<tr><td>get-erm-for-empty-epoch</td><td>If true, Purging will get the EpochRecoveryMetadata even if the epoch is empty locally</td><td style="text-align:center">true</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>max-active-cached-digests</td><td>maximum number of active cached digest streams on a storage node at the same time</td><td style="text-align:center">2000</td><td>requires restart, server only</td></tr>
<tr><td>max-cached-digest-record-queued-kb</td><td>amount of RECORD data to push to the client at once for cached digesting</td><td style="text-align:center">256</td><td>requires restart, server only</td></tr>
<tr><td>max-concurrent-purging-for-release-per-shard</td><td>max number of concurrently running purging state machines for RELEASE messages per each storage shard for each worker</td><td style="text-align:center">4</td><td>requires restart, server only</td></tr>
<tr><td>mutation-timeout</td><td>initial timeout used during the mutation phase of log recovery to store enough copies of a record or a hole plug</td><td style="text-align:center">500ms</td><td>server only</td></tr>
<tr><td>purging-use-metadata-log-only</td><td>If true, the NodeSetFinder within PurgeUncleanEpochs will useonly the metadata log as source for fetching historical metadata.used only for migration</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>record-cache-max-size</td><td>Maximum size enforced for the record cache, 0 for unlimited. If positive and record cache size grows more than that, it will start evicting records from the cache. This is also the maximum total number of bytes allowed to be persisted in record cache snapshots. For snapshot limit, this is enforced per-shard with each shard having its own limit of (max_record_cache_snapshot_bytes / num_shards).</td><td style="text-align:center">4294967296</td><td>server only</td></tr>
<tr><td>record-cache-monitor-interval</td><td>polling interval for the record cache eviction thread for monitoring the size of the record cache.</td><td style="text-align:center">2s</td><td>server only</td></tr>
<tr><td>recovery-grace-period</td><td>Grace period time used by epoch recovery after it acquires an authoritative incomplete digest but wants to wait more time for an authoritative complete digest. Millisecond granularity. Can be 0.</td><td style="text-align:center">100ms</td><td>server only</td></tr>
<tr><td>recovery-seq-metadata-timeout</td><td>Retry backoff timeout used for checking if the latest metadata log record is fully replicated during log recovery.</td><td style="text-align:center">2s..60s</td><td>server only</td></tr>
<tr><td>recovery-timeout</td><td>epoch recovery timeout. Millisecond granularity.</td><td style="text-align:center">120s</td><td>server only</td></tr>
<tr><td>single-empty-erm</td><td>A single E:EMPTY response for an epoch is sufficient for GetEpochRecoveryMetadataRequest to consider the epoch as empty if this option is set.</td><td style="text-align:center">true</td><td><strong>experimental</strong>, server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="resource-management"></a><a href="#resource-management" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Resource management</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>eagerly-allocate-fdtable</td><td>enables an optimization to eagerly allocate the kernel fdtable at startup</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>fd-limit</td><td>maximum number of file descriptors that the process can allocate (may require root priviliges). If equal to zero, do not set any limit.</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>flow-groups-run-deadline</td><td>Maximum delay (plus one cycle of the event loop) between a request to run FlowGroups and Sender::runFlowGroups() executing.</td><td style="text-align:center">5ms</td><td></td></tr>
<tr><td>flow-groups-run-yield-interval</td><td>Maximum duration of Sender::runFlowGroups() before yielding to the event loop.</td><td style="text-align:center">2ms</td><td></td></tr>
<tr><td>lock-memory</td><td>On startup, call mlockall() to lock the text segment (executable code) of logdeviced in RAM.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>max-inflight-storage-tasks</td><td>max number of StorageTask instances that one worker thread may have in flight to each database shard</td><td style="text-align:center">4096</td><td>requires restart, server only</td></tr>
<tr><td>max-payload-size</td><td>The maximum payload size that will be accepted by the client library or the server. Can't be larger than 33554432 bytes.</td><td style="text-align:center">1048576</td><td></td></tr>
<tr><td>max-record-read-execution-time</td><td>Maximum execution time for reading records. 'max' means no limit.</td><td style="text-align:center">max</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>max-server-read-streams</td><td>max number of read streams clients can establish to the server, per worker</td><td style="text-align:center">150000</td><td>server only</td></tr>
<tr><td>max-total-appenders-size-hard</td><td>Total size in bytes of running Appenders accross all workers after which we start rejecting new appends.</td><td style="text-align:center">629145600</td><td>server only</td></tr>
<tr><td>max-total-appenders-size-soft</td><td>Total size in bytes of running Appenders accross all workers after which we start taking measures to reduce the Appender residency time.</td><td style="text-align:center">524288000</td><td>server only</td></tr>
<tr><td>max-total-buffered-append-size</td><td>Total size in bytes of payloads buffered in BufferedWriters in sequencers for server-side batching and compression. Appends will be rejected when this threshold is significantly exceeded.</td><td style="text-align:center">1073741824</td><td>server only</td></tr>
<tr><td>num-reserved-fds</td><td>expected number of file descriptors to reserve for use by RocksDB files and server-to-server connections within the cluster. This number is subtracted from --fd-limit (if set) to obtain the maximum number of client TCP connections that the server will be willing to accept.</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>per-worker-storage-task-queue-size</td><td>max number of StorageTask instances to buffer in each Worker for each local log store shard</td><td style="text-align:center">16384</td><td>requires restart, server only</td></tr>
<tr><td>queue-drop-overload-time</td><td>max time after worker's storage task queue is dropped before it stops being considered overloaded</td><td style="text-align:center">1s</td><td>server only</td></tr>
<tr><td>queue-size-overload-percentage</td><td>percentage of per-worker-storage-task-queue-size that can be buffered before the queue is considered overloaded</td><td style="text-align:center">50</td><td>server only</td></tr>
<tr><td>read-storage-tasks-max-mem-bytes</td><td>Maximum amount of memory that can be allocated by read storage tasks.</td><td style="text-align:center">16106127360</td><td>server only</td></tr>
<tr><td>rocksdb-low-ioprio</td><td>IO priority to request for low-pri rocksdb threads. This works only if current IO scheduler supports IO priorities.See man ioprio_set for possible values. &quot;any&quot; or &quot;&quot; to keep the default.</td><td style="text-align:center">3,0</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-stall-cache-ttl</td><td>How often to re-check whether we should stall low-pri writes</td><td style="text-align:center">100ms</td><td>server only</td></tr>
<tr><td>slow-ioprio</td><td>IO priority to request for 'slow' storage threads. Storage threads in the 'slow' thread pool handle high-latency RocksDB IO requests,  primarily data reads. Not all kernel IO schedulers supports IO priorities.See man ioprio_set for possible values.&quot;any&quot; or &quot;&quot; to keep the default.</td><td style="text-align:center">3,0</td><td>requires restart, server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="rocksdb"></a><a href="#rocksdb" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RocksDB</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>iterator-cache-ttl</td><td>expiration time of idle RocksDB iterators in the iterator cache.</td><td style="text-align:center">20s</td><td>server only</td></tr>
<tr><td>rocksdb-advise-random-on-open</td><td>if true, will hint the underlying file system that the file access pattern is random when an SST file is opened</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-allow-fallocate</td><td>If false, fallocate() calls are bypassed in rocksdb</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-arena-block-size</td><td>granularity of memtable allocations</td><td style="text-align:center">4194304</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-block-size</td><td>approximate size of the uncompressed data block; rocksdb memory usage for index is around [total data size] / block_size * 50 bytes; on HDD consider using a much bigger value to keep memory usage reasonable</td><td style="text-align:center">500K</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-bloom-bits-per-key</td><td>Controls the size of bloom filters in sst files. Set to 0 to disable bloom filters. &quot;Key&quot; in the bloom filter is log ID and entry type (data record, CSI entry or findTime index entry). Iterators then use this information to skip files that don't contain any records of the requested log. The default value of 10 corresponds to false positive rate of ~1%. Note that LogsDB already skips partitions that don't have the requested logs, so bloom filters only help for somewhat bursty write patterns - when only a subset of files in a partition contain a given log. However, even if appends to a log are steady, sticky copysets may make the streams of STOREs to individual nodes bursty.</td><td style="text-align:center">10</td><td>server only</td></tr>
<tr><td>rocksdb-bloom-block-based</td><td>If true, rocksdb will use a separate bloom filter for each block of sst file. These small bloom filters will be at least 9 bytes each (even if bloom-bits-per-key is smaller). For data records, usually each block contains only one log, so the bloom filter size will be around max(72, bloom_bits_per_key) + 2 * bloom_bits_per_key  per log per sst (the &quot;2&quot; corresponds to CSI and findTime index entries; if one or both is disabled, it's correspondingly smaller).</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rocksdb-bytes-per-sync</td><td>when writing files (except WAL), sync once per this many bytes written. 0 turns off incremental syncing, the whole file will be synced after it's written</td><td style="text-align:center">1048576</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-bytes-written-since-flush-trigger</td><td>The maximum amount of buffered writes which will be accumulated before write data is flushed to stable storage. 0 disables the trigger</td><td style="text-align:center">0</td><td>server only</td></tr>
<tr><td>rocksdb-cache-high-pri-pool-ratio</td><td>Ratio of rocksdb block cache reserve for index and filter blocks, if --rocksdb-cache-index-with-high-priority is enabled.</td><td style="text-align:center">0.0</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-cache-index</td><td>put index and filter blocks in the block cache, allowing them to be evicted</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-cache-index-with-high-priority</td><td>Cache index and filter block in high pri pool of block cache, making them less likely to be evicted than data blocks.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-cache-numshardbits</td><td>This setting is not important. Width in bits of the number of shards into which to partition the uncompressed block cache. See rocksdb/cache.h.</td><td style="text-align:center">4</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-cache-size</td><td>size of uncompressed RocksDB block cache</td><td style="text-align:center">10G</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-compaction-access-sequential</td><td>suggest to the OS that input files will be accessed sequentially during compaction</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-compaction-ratelimit</td><td>limits how fast compactions can read uncompressed data, in bytes; format is <count><suffix>/<duration><unit>. Example: 5M/500ms means compaction will read 5MB per 500ms. This is applied to each compaction independently (e.g. if multiple shards are compacting simultaneously the total rate can be over the limit). Unlimited by default. IMPORTANT: This limits the rate of uncompressed data. If rocksdb compressed data 2X, the actual disk read rate will be around 1/2 of this limit.</td><td style="text-align:center">30M/1s</td><td>server only</td></tr>
<tr><td>rocksdb-compaction-readahead-size</td><td>if non-zero, perform reads of this size (in bytes) when doing compaction; big readahead can decrease efficiency of compactions that remove a lot of records (compaction skips trimmed records using seeks)</td><td style="text-align:center">4096</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-compaction-style</td><td>compaction style: 'universal' (default) or 'level'; if using 'level', also set --num-levels to at least 2</td><td style="text-align:center">universal</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-compressed-cache-numshardbits</td><td>This setting is not important. Width in bits of the number of shards into which to partition the compressed block cache, if enabled. See rocksdb/cache.h.</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-compressed-cache-size</td><td>size of compressed RocksDB block cache (0 to turn off)</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-compression-type</td><td>compression algorithm: 'snappy' (default), 'none', 'zlib', 'bzip2', 'lz4', 'lz4hc', 'zstd'</td><td style="text-align:center">none</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-db-write-buffer-size</td><td>Soft limit on the total size of memtables per shard; when exceeded, oldest memtables will automatically be flushed. This may soon be superseded by a more global --rocksdb-memtable-size-per-node limit that should be set to &lt;num_shards&gt; * what you'd set this to.</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-disable-iterate-upper-bound</td><td>disable iterate_upper_bound optimization in RocksDB</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rocksdb-enable-insert-hint</td><td>Enable rocksdb insert hint optimization. May reduce CPU usage for inserting keys into rocksdb, with small memory overhead.</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-enable-statistics</td><td>if set, instruct RocksDB to collect various statistics</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-flush-block-policy</td><td>Controls how RocksDB splits SST file data into blocks. 'default' starts a new block when --rocksdb-block-size is reached. 'each_log', in addition to what 'default' does, starts a new block when log ID changes. 'each_copyset', in addition to what 'each_log' does, starts a new block when copyset changes. Both 'each_*' don't start a new block if current block is smaller than --rocksdb-min-block-size. 'each_log' should be safe to use in all cases. 'each_copyset' should only be used when sticky copysets are enabled with --write-sticky-copysets (otherwise it would start a block for almost every record).</td><td style="text-align:center">each_log</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-index-block-restart-interval</td><td>Number of keys between restart points for prefix encoding of keys in index blocks.  Typically one of two values: 1 for no prefix encoding, 16 for prefix encoding (smaller memory footprint of the index).</td><td style="text-align:center">16</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-level0-file-num-compaction-trigger</td><td>trigger L0 compaction at this many L0 files. This applies to the unpartitioned and metadata column families only, not to LogsDB data partitions.</td><td style="text-align:center">10</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-level0-slowdown-writes-trigger</td><td>start throttling writers at this many L0 files. This applies to the unpartitioned and metadata column families only, not to LogsDB data partitions.</td><td style="text-align:center">25</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-level0-stop-writes-trigger</td><td>stop accepting writes (block writers) at this many L0 files. This applies to the unpartitioned and metadata column families only, not to LogsDB data partitions.</td><td style="text-align:center">30</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-max-background-compactions</td><td>Maximum number of concurrent rocksdb-initiated background compactions per shard. Note that this value is not important since most compactions are not &quot;background&quot; as far as rocksdb is concerned. They're done from _logsdb_ thread and are limited to one per shard at a time, regardless of this option.</td><td style="text-align:center">2</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-max-background-flushes</td><td>maximum number of concurrent background memtable flushes per shard. Flushes run on the rocksdb hipri thread pool</td><td style="text-align:center">15</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-max-bytes-for-level-base</td><td>maximum combined data size for L1</td><td style="text-align:center">10G</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-max-bytes-for-level-multiplier</td><td>L_n -&gt; L_n+1 data size multiplier</td><td style="text-align:center">8</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-max-open-files</td><td>maximum number of concurrently open RocksDB files; -1 for unlimited</td><td style="text-align:center">10000</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-max-write-buffer-number</td><td>maximum number of concurrent write buffers</td><td style="text-align:center">2</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-memtable-size-per-node</td><td>soft limit on the total size of memtables per node; when exceeded, oldest memtable in the shard whose growth took the total memory usage over the threshold will automatically be flushed. This is a soft limit in the sense that flushing may fall behind or freeing memory be delayed for other reasons, causing us to exceed the limit. --rocksdb-db-write-buffer-size overrides this if it is set, but it will be deprecated eventually.</td><td style="text-align:center">10G</td><td>requires restart, <strong>experimental</strong>, server only</td></tr>
<tr><td>rocksdb-metadata-block-size</td><td>approximate size of the uncompressed data block for metadata column family (if --rocksdb-partitioned); if zero, same as --rocksdb-block-size</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-metadata-bloom-bits-per-key</td><td>Similar to --rocksdb-bloom-bits-per-key but for metadata column family. You probably don't want to enable this. This option is here just for completeness. It's not expected to have any positive effect since almost all reads from metadata column family bypass bloom filters (with total_order_seek = true).</td><td style="text-align:center">0</td><td>server only</td></tr>
<tr><td>rocksdb-metadata-cache-numshardbits</td><td>This setting is not important. Width in bits of the number of shards into which to partition the uncompressed block cache for metadata. See rocksdb/cache.h.</td><td style="text-align:center">4</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-metadata-cache-size</td><td>size of uncompressed RocksDB block cache for metadata</td><td style="text-align:center">1G</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-min-block-size</td><td>minimum size of the uncompressed data block; only used when --rocksdb-flush-block-policy is not default; on SSD consider reducing this value</td><td style="text-align:center">16384</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-min-manual-flush-interval</td><td>How often a background thread will flush buffered writes if either the data age, partition idle, or data amount triggers indicate a flush should occur. 0 disables all manual flushes</td><td style="text-align:center">120s</td><td>server only</td></tr>
<tr><td>rocksdb-num-bg-threads-hi</td><td>Number of high-priority rocksdb background threads to run. These threads are shared among all shards. If -1, num_shards * max_background_flushes is used.</td><td style="text-align:center">-1</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-num-bg-threads-lo</td><td>Number of low-priority rocksdb background threads to run. These threads are shared among all shards. If -1, num_shards * max_background_compactions is used.</td><td style="text-align:center">-1</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-num-levels</td><td>number of LSM-tree levels if level compaction is used</td><td style="text-align:center">1</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-partition-data-age-flush-trigger</td><td>Maximum wait after data are written before being flushed to stable storage. 0 disables the trigger.</td><td style="text-align:center">600s</td><td>server only</td></tr>
<tr><td>rocksdb-partition-idle-flush-trigger</td><td>Maximum wait after writes to a time partition cease before any uncommitted data are flushed to stable storage. 0 disables the trigger.</td><td style="text-align:center">300s</td><td>server only</td></tr>
<tr><td>rocksdb-read-amp-bytes-per-bit</td><td>If greater than 0, will create a bitmap to estimate rocksdb read amplification and expose the result through READ_AMP_ESTIMATE_USEFUL_BYTES and READ_AMP_TOTAL_READ_BYTES stats.</td><td style="text-align:center">32</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-skip-list-lookahead</td><td>number of keys to examine in the neighborhood of the current key when searching within a skiplist (0 to disable the optimization)</td><td style="text-align:center">3</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-sst-delete-bytes-per-sec</td><td>ratelimit in bytes/sec on deletion of SST files per shard; 0 for unlimited.</td><td style="text-align:center">0</td><td>server only</td></tr>
<tr><td>rocksdb-target-file-size-base</td><td>target L1 file size for compaction</td><td style="text-align:center">67108864</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-uc-max-merge-width</td><td>maximum number of files in a single universal compaction run</td><td style="text-align:center">4294967295</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-uc-max-size-amplification-percent</td><td>target size amplification percentage for universal compaction</td><td style="text-align:center">200</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-uc-min-merge-width</td><td>minimum number of files in a single universal compaction run</td><td style="text-align:center">2</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-uc-size-ratio</td><td>arg is a percentage. If the candidate set size for compaction is arg% smaller than the next file size, then include next file in the candidate set.</td><td style="text-align:center">1M</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-update-stats-on-db-open</td><td>load stats from property blocks of several files when opening the database in order to optimize compaction decisions. May significantly impact the time needed to open the db.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-wal-bytes-per-sync</td><td>when writing WAL, sync once per this many bytes written. 0 turns off incremental syncing</td><td style="text-align:center">1M</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-write-buffer-size</td><td>When any RocksDB memtable ('write buffer') reaches this size it is made immitable, then flushed into a newly created L0 file. This setting may soon be superceded by a more dynamic --memtable-size-per-node limit.</td><td style="text-align:center">100G</td><td>requires restart, server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="security"></a><a href="#security" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Security</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>audit-log</td><td>Path for log file storing information about all trim point changes. For log rotation using logrotate send SIGHUP to process after rotation to reopen the log.</td><td style="text-align:center"></td><td>requires restart, server only</td></tr>
<tr><td>require-ssl-on-command-port</td><td>Requires SSL for admin commands sent to the command port. --ssl-cert-path, --ssl-key-path and --ssl-ca-path settings must be properly configured</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>ssl-boundary</td><td>Enable SSL in cross-X traffic, where X is the setting. Example: if set to &quot;rack&quot;, all cross-rack traffic will be sent over SSL. Can be one of &quot;none&quot;, &quot;node&quot;, &quot;rack&quot;, &quot;row&quot;, &quot;cluster&quot;, &quot;data_center&quot; or &quot;region&quot;. If a value other than &quot;none&quot; or &quot;node&quot; is specified on the client, --my-location has to be specified as well.</td><td style="text-align:center">none</td><td></td></tr>
<tr><td>ssl-ca-path</td><td>Path to CA certificate.</td><td style="text-align:center"></td><td>requires restart</td></tr>
<tr><td>ssl-cert-path</td><td>Path to LogDevice SSL certificate.</td><td style="text-align:center"></td><td>requires restart</td></tr>
<tr><td>ssl-cert-refresh-interval</td><td>TTL for an SSL certificate that we have loaded from disk.</td><td style="text-align:center">300s</td><td>requires restart</td></tr>
<tr><td>ssl-key-path</td><td>Path to LogDevice SSL key.</td><td style="text-align:center"></td><td>requires restart</td></tr>
<tr><td>ssl-load-client-cert</td><td>Set to include client certificate for mutual ssl authenticaiton</td><td style="text-align:center">false</td><td></td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="sequencer-state"></a><a href="#sequencer-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Sequencer State</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>check-seal-req-min-timeout</td><td>before a sequencer returns its state in response to a 'get sequencer state' request the sequencer checks that it is the most recent (highest numbered) sequencer for the log. It performs the check by sending a 'check seal' request to a valid copyset of nodes in the nodeset of the sequencer's epoch. The 'check seal' request looks for a seal record placed by a higher-numbered sequencer. This setting sets the timeout for 'check seal' requests. The timeout is set to the smaller of this value and half the value of --seq-state-reply-timeout.</td><td style="text-align:center">500ms</td><td>server only</td></tr>
<tr><td>epoch-draining-timeout</td><td>Maximum time allowed for sequencer to drain one epoch. Sequencer will abort draining the epoch if it takes longer than the timeout. A sequencer 'drains' its epoch (waits for all appenders to complete) while reactivating to serve a higher epoch.</td><td style="text-align:center">2s</td><td>server only</td></tr>
<tr><td>get-trimpoint-interval</td><td>polling interval for the sequencer getting the trim point from all storage nodes</td><td style="text-align:center">600s</td><td>server only</td></tr>
<tr><td>reactivation-limit</td><td>Maximum allowed rate of sequencer reactivations. When exceeded, further appends will fail.</td><td style="text-align:center">5/1s</td><td>requires restart, server only</td></tr>
<tr><td>read-historical-metadata-timeout</td><td>maximum time interval for a sequencer to get historical epoch metadata through reading the metadata log before retrying.</td><td style="text-align:center">10s</td><td>server only</td></tr>
<tr><td>seq-state-backoff-time</td><td>how long to wait before resending a 'get sequencer state' request after a timeout.</td><td style="text-align:center">1s..10s</td><td></td></tr>
<tr><td>seq-state-reply-timeout</td><td>how long to wait for a reply to a 'get sequencer state' request before retrying (usually to a different node)</td><td style="text-align:center">2s</td><td></td></tr>
<tr><td>update-metadata-map-interval</td><td>Sequencer has a timer for periodically reading metadata logs and refreshing the in memory metadata_map_. This setting specifies the interval for this timer</td><td style="text-align:center">1h</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="sequencer-boycotting"></a><a href="#sequencer-boycotting" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Sequencer boycotting</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>boycotts-observe-only</td><td>If true, the entire system of detecting append success ratio outliers and performing boycotts will continue to work as expected, with stats getting updated and boycotts propagating with gossip, but will no longer affect sequencer placement. Used to be able to observe how the feature works without committing.</td><td style="text-align:center">false</td><td><strong>experimental</strong></td></tr>
<tr><td>node-stats-boycott-duration</td><td>How long a boycott should be active for. 0 will ensure that boycotts has no effect, but controller nodes will still report outliers</td><td style="text-align:center">0s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-boycott-grace-period</td><td>If a node is an consecutively deemed an outlier for this amount of time, allow it to be boycotted</td><td style="text-align:center">300s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-boycott-relative-margin</td><td>If this is set to 0.05, a node's append success ratio has to be 5% smaller than the average success ratio of all nodes in the cluster. While node-stats-boycott-sensitivity is an absolute threshold, this setting defines a sensitivity threshold relative to the average of all success ratios. Only used if node-stats-boycott-use-rmsd is true</td><td style="text-align:center">0.15</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-boycott-required-client-count</td><td>Require at least values from this many clients before a boycott may occur</td><td style="text-align:center">1</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-boycott-required-std-from-mean</td><td>A node has to have a success ratio lower than (mean - X * STD) to be considered an outlier. X being the value of node-stats-boycott-required-std-from-mean</td><td style="text-align:center">3</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-boycott-sensitivity</td><td>If node-stats-boycott-sensitivity is set to e.g. 0.05, then nodes with a success ratio at or above 95% will not be boycotted</td><td style="text-align:center">0</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-boycott-use-rmsd</td><td>(experimental) Use a new outlier detection algorithm</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-controller-aggregation-period</td><td>The period at which the controller nodes requests stats from all nodes in the cluster. Should be smaller than node-stats-retention-on-nodes</td><td style="text-align:center">30s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-controller-check-period</td><td>A node will check if it's a controller or not with the given period</td><td style="text-align:center">60s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-controller-response-timeout</td><td>A controller node waits this long between requesting stats from the other nodes, and aggregating the received stats</td><td style="text-align:center">2s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-max-boycott-count</td><td>How many nodes may be boycotted. 0 will in addition to not allowing any nodes to be boycotted, it also ensures no nodes become controller nodes</td><td style="text-align:center">0</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-remove-worst-percentage</td><td>Will throw away the worst X% of values reported by clients, to a maximum of node-count * node-stats-send-worst-client-count</td><td style="text-align:center">0.2</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-retention-on-nodes</td><td>Save node stats sent from the clients on the nodes for this duration</td><td style="text-align:center">300s</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-send-period</td><td>Send per-node stats into the cluster with this period. Currently only 30s of stats is tracked on the clients, so a value above 30s will not have any effect.</td><td style="text-align:center">15s</td><td><strong>experimental</strong>, client only</td></tr>
<tr><td>node-stats-send-retry-delay</td><td>When sending per-node stats into the cluster, and the message failed, wait this much before retrying.</td><td style="text-align:center">5ms..1s</td><td>requires restart, <strong>experimental</strong>, client only</td></tr>
<tr><td>node-stats-send-worst-client-count</td><td>Once a node has aggregated the values sent from writers, there may be some amount of writers that are in a bad state and report 'false' values. By setting this value, the <code>node-stats-send-worst-client-count</code> worst values reported by clients per node will be sent separately to the controller, which can then take a decision if the writer is functioning correctly or not.</td><td style="text-align:center">20</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>node-stats-timeout-delay</td><td>Wait this long for an acknowledgement that the sent node stats message was received before sending the stats to another node</td><td style="text-align:center">2s</td><td><strong>experimental</strong>, client only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="state-machine-execution"></a><a href="#state-machine-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>State machine execution</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>background-queue-size</td><td>Maximum number of events we can queue to background thread.  A single queue is shared by all threads in a process.</td><td style="text-align:center">100000</td><td>requires restart</td></tr>
<tr><td>execute-requests</td><td>number of requests to process per worker event loop iteration</td><td style="text-align:center">16</td><td></td></tr>
<tr><td>num-background-workers</td><td>The number of workers dedicated for processing time-insensitive requests and operations</td><td style="text-align:center">4</td><td>requires restart, server only</td></tr>
<tr><td>num-processor-background-threads</td><td>Number of threads in Processor's background thread pool. Background threads are used by, e.g., BufferedWriter to construct/compress large batches.  If 0 (default), use num-workers.</td><td style="text-align:center">0</td><td>requires restart</td></tr>
<tr><td>num-workers</td><td>number of worker threads to run, or &quot;cores&quot; for one thread per CPU core</td><td style="text-align:center">cores</td><td>requires restart</td></tr>
<tr><td>worker-request-pipe-capacity</td><td>size each worker request queue to hold this many requests</td><td style="text-align:center">524288</td><td>requires restart</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="storage"></a><a href="#storage" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Storage</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>free-disk-space-threshold</td><td>threshold (relative to total diskspace) of minimal free disk space for storage partitions to accept writes. This should be a fraction between 0.0 (exclusive) and 1.0 (exclusive). Storage nodes will reject writes to partitions with free disk space less than the threshold in order to guarantee that compactions can be performed. Note: this only applies to RocksDB local storage.</td><td style="text-align:center">0.2</td><td>server only</td></tr>
<tr><td>ignore-cluster-marker</td><td>If cluster marker is missing or doesn't match, overwrite it and carry on. Cluster marker is a file that LogsDB writes in the data directory of each shard to identify the shard id, node id, and cluster name to which the data in that directory belongs. Cluster marker mismatch indicates that a drive or node was moved to another cluster or another shard, and the data must not be used.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>local-log-store-path</td><td>path to local log store (if storage node)</td><td style="text-align:center"></td><td>requires restart, server only</td></tr>
<tr><td>logstore-monitoring-interval</td><td>interval between consecutive health checks on the local log store</td><td style="text-align:center">10s</td><td>server only</td></tr>
<tr><td>max-in-flight-monitor-requests</td><td>maximum number of in-flight monitoring requests (e.g. manual compaction) posted by the monitoring thread</td><td style="text-align:center">1</td><td>requires restart, server only</td></tr>
<tr><td>max-queued-monitor-requests</td><td>max number of log store monitor requests buffered in the monitoring thread queue</td><td style="text-align:center">32</td><td>requires restart, server only</td></tr>
<tr><td>rocksdb-auto-create-shards</td><td>Auto-create shard data directories if they do not exist</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>storage-threads-per-shard-fast</td><td>size of the 'fast' storage thread pool, per shard. This storage thread pool executes storage tasks that write into RocksDB. Such tasks normally do not block on IO. If zero, slow threads will handle write tasks.</td><td style="text-align:center">2</td><td>requires restart, server only</td></tr>
<tr><td>storage-threads-per-shard-fast-stallable</td><td>size of the thread pool (per shard) executing low priority write tasks, such as writing rebuilding records into RocksDB. Measures are taken to not schedule low-priority writes on this thread pool when there is work for 'fast' threads. If zero, normal fast threads will handle low-pri write tasks</td><td style="text-align:center">1</td><td>requires restart, server only</td></tr>
<tr><td>storage-threads-per-shard-metadata</td><td>size of the storage thread pool for metadata writes, per shard. If zero, the 'slow' pool will handle metadata writing tasks.</td><td style="text-align:center">2</td><td>requires restart, server only</td></tr>
<tr><td>storage-threads-per-shard-slow</td><td>size of the 'slow' storage thread pool, per shard. This storage thread pool executes storage tasks that read log records from RocksDB, both to serve read requests from clients, and for rebuilding. Those are likely to block on IO.</td><td style="text-align:center">2</td><td>requires restart, server only</td></tr>
<tr><td>write-batch-bytes</td><td>min number of payload bytes for a storage thread to write in one batch unless write-batch-size is reached first</td><td style="text-align:center">1048576</td><td>server only</td></tr>
<tr><td>write-batch-size</td><td>max number of records for a storage thread to write in one batch</td><td style="text-align:center">1024</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="testing"></a><a href="#testing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Testing</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>abort-on-failed-catch</td><td>When an ld_catch() fails, call abort().  If not, just continue executing.  We'll log either way.</td><td style="text-align:center">true</td><td></td></tr>
<tr><td>abort-on-failed-check</td><td>When an ld_check() fails, call abort().  If not, just continue executing.  We'll log either way.</td><td style="text-align:center">true</td><td></td></tr>
<tr><td>assert-on-data</td><td>Trigger asserts on data in RocksDB (or that received from the network). Should not be used in prod.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>client-test-force-stats</td><td>force instantiation of StatsHolder within ClientImpl even if stats publishing is disabled</td><td style="text-align:center">false</td><td>requires restart, client only</td></tr>
<tr><td>command-unix-socket</td><td>Path to the unix domain socket the server will use to listen for admin commands, supports commands over SSL</td><td style="text-align:center"></td><td>requires restart, server only</td></tr>
<tr><td>disable-event-log-trimming</td><td>Disable trimming of the event log (for tests only)</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>disable-logsconfig-trimming</td><td>Disable the trimming of logsconfig delta log. Used for testing only.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>disable-rebuilding</td><td>Disable rebuilding. Do not use in production. Only used by tests.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>disable-trim-past-tail-check</td><td>Disable check for trim past tail. Used for testing log trimming.</td><td style="text-align:center">false</td><td>client only</td></tr>
<tr><td>dont-serve-findtimes-for-logs</td><td>Logs for which findtimes will not be served</td><td style="text-align:center"></td><td>server only</td></tr>
<tr><td>dont-serve-findtimes-status</td><td>status that should be returned for logs that are in &quot;dont-serve-findtimes-for-logs&quot;</td><td style="text-align:center">FAILED</td><td>server only</td></tr>
<tr><td>dont-serve-reads-for-logs</td><td>Logs for which reads will not be served</td><td style="text-align:center"></td><td>server only</td></tr>
<tr><td>dont-serve-reads-status</td><td>status that should be returned for logs that are in &quot;dont-serve-reads-for-logs&quot;</td><td style="text-align:center">FAILED</td><td>server only</td></tr>
<tr><td>dont-serve-stores-for-logs</td><td>Logs for which stores will not be served</td><td style="text-align:center"></td><td>server only</td></tr>
<tr><td>dont-serve-stores-status</td><td>status that should be returned for logs that are in &quot;dont-serve-stores-for-logs&quot;</td><td style="text-align:center">FAILED</td><td>server only</td></tr>
<tr><td>epoch-store-path</td><td>directory containing epoch files for logs (for testing only)</td><td style="text-align:center"></td><td>requires restart, server only</td></tr>
<tr><td>esn-bits</td><td>How many bits to use for sequence numbers within an epoch.  LSN bits [n, 32) are guaranteed to be 0. Used for testing ESN exhaustion.</td><td style="text-align:center">32</td><td>requires restart, server only</td></tr>
<tr><td>hold-store-replies</td><td>If set, we hold all STORED messages (which are replies to STORE messages), until the last one comes is.  Has some race conditions and other down sides, so only use in tests.  Used to ensure that all storage nodes have had a chance to process the STORE messages, even if one returns PREEMPTED or another error condition.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>include-cluster-name-on-handshake</td><td>The cluster name of the connection initiator will be included in the LogDevice protocol handshake. If the cluster name of the initiator does not match the actual cluster name of the destination, the connection is terminated. We don't know of any good reasons to disable this option. If you disable it and move some hosts from one cluster to another, you may have a bad time: some clients or servers may not pick up the update and keep talking to the hosts as if they weren't moved; this may corrupt metadata. Used for testing and internally created connections only.</td><td style="text-align:center">true</td><td></td></tr>
<tr><td>loglevel-overrides</td><td>Comma-separated list of <module>:<loglevel>. eg: Server.cpp:debug,Sequencer.cpp:notify</td><td style="text-align:center"></td><td>server only</td></tr>
<tr><td>msg-error-injection-chance</td><td>percentage chance of a forced message error on a Socket. Used to exercise error handling paths.</td><td style="text-align:center">0</td><td>requires restart, server only</td></tr>
<tr><td>msg-error-injection-status</td><td>status that should be returned for a simulated message transmission error</td><td style="text-align:center">NOBUFS</td><td>requires restart</td></tr>
<tr><td>rebuild-without-amends</td><td>During rebuilding, send a normal STORE rather than a STORE with the AMEND flag, when updating the copyset of nodes that already have a copy of the record. This option is used by integration tests to fully divorce append content from records touched by rebuilding.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>rebuilding-read-only</td><td>Rebuilding is read-only (for testing only). Use on-donor if rebuilding should not send STORE messages, or on-recipient if these should be sent but discarded by the recipient (LogsDB only)</td><td style="text-align:center">none</td><td>server only</td></tr>
<tr><td>rebuilding-retry-timeout</td><td>Delay before a record rebuilding retries a failed operation</td><td style="text-align:center">5s..30s</td><td>server only</td></tr>
<tr><td>rocksdb-test-corrupt-stores</td><td>Used for testing only. If true, a node will report all stores it receives as corrupted.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>skip-recovery</td><td>Skip recovery. For tests only. When this option is enabled, recovery does not recover any data but instead immediately marks all epochs as clean in the epoch store and purging immediately marks all epochs as clean in the local log store. This feature should be used as a last resort if a cluster's availability is hurt by recovery and it is important to quickly restore availability at the cost of some inconsistencies. On-the-fly changes of this setting will only apply to new LogRecoveryRequests and will not affect recoveries that are already in progress.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>test-appender-skip-stores</td><td>Allow appenders to skip sending data to storage node. Currently used in tests to make sure an appender state machine is running</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>test-bypass-recovery</td><td>If set, sequencers will not automatically run recovery upon activation. Recovery can be started using the 'startrecovery' admin command.  Note that last released lsn won't get advanced without recovery.</td><td style="text-align:center">false</td><td>requires restart, server only</td></tr>
<tr><td>test-do-not-pick-in-copysets</td><td>Copyset selectors won't pick these nodes. Comma-separated list of node indexes, e.g. '1,2,3'. Used in tests.</td><td style="text-align:center"></td><td>server only</td></tr>
<tr><td>test-get-cluster-state-recipients</td><td>Force get-cluster-state recipients as a comma-separated list of node ids</td><td style="text-align:center"></td><td>client only</td></tr>
<tr><td>test-reject-hello</td><td>if set to the name of an error code, reject all HELLOs with the specified error code. Currently supported values are ACCESS and PROTONOSUPPORT. Used for testing.</td><td style="text-align:center">OK</td><td>requires restart, server only</td></tr>
<tr><td>test-sequencer-corrupt-stores</td><td>Simulates bad hardware flipping a bit in the payload of a STORE message.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>test-stall-rebuilding</td><td>Makes rebuilding pretend to start but not make any actual progress. Used in tests.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>test-timestamp-linear-transform</td><td>Coefficents for tranforming the timestamp of records for test. The value should contain two integrs sperated by ','. For example'm,c'. Records timestamp is tranformed as m * now() + c.A default value of '1,0' makes the timestamp = now() which is expectedfor all the normal use cases.</td><td style="text-align:center">1,0</td><td>requires restart, server only</td></tr>
<tr><td>unix-socket</td><td>Path to the unix domain socket the server will use to listen for non-SSL clients</td><td style="text-align:center"></td><td>CLI only, requires restart, server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="uncategorized"></a><a href="#uncategorized" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Uncategorized</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>client-readers-flow-tracer-lagging-metric-num-sample-groups</td><td>Maximum number of samples that are kept by ClientReadersFlowTracer for computing relative reading speed in relation to writing speed. See client_readers_flow_tracer_lagging_slope_threshold.</td><td style="text-align:center">3</td><td>client only</td></tr>
<tr><td>client-readers-flow-tracer-lagging-metric-sample-group-size</td><td>Number of samples in ClientReadersFlowTracer that are aggregated and recorded as one entry. See client-readers-flow-tracer-lagging-metric-sample-group-size.</td><td style="text-align:center">20</td><td>client only</td></tr>
<tr><td>client-readers-flow-tracer-lagging-slope-threshold</td><td>If a reader's lag increase at at least this rate, the reader is considered lagging (rate given as variation of time lag per time unit). If the desired read ratio needs to be x% of the write ratio, set this threshold to be (1 - x / 100).</td><td style="text-align:center">-0.3</td><td>client only</td></tr>
<tr><td>enable-adaptive-store-timeout</td><td>decides whether to enable an adaptive store timeout</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>rebuilding-max-batch-time</td><td>Max amount of time rebuilding read storage task is allowed to take before yielding to other storage tasks. Only supported by rebuilding V2 (partition by partition). &quot;max&quot; for unlimited.</td><td style="text-align:center">1000ms</td><td>server only</td></tr>
<tr><td>rebuilding-max-malformed-records-to-tolerate</td><td>Controls how rebuilding donors handle unexpected values in local log store (e.g. caused by bugs, forward incompatibility, or other processes writing unexpected things to rocksdb directly).If rebuilding encounters invalid records, it skips them and logs warnings. But if it encounters at least this many of them in the same log, it freaks out, logs a critical error and stalls indefinitely. The rest of the server keeps trying to run normally, to the extent to which you can run normally when you can't parse most of the records in the DB.</td><td style="text-align:center">1000</td><td>requires restart, server only</td></tr>
<tr><td>sync-metadata-log-writes</td><td>If set, storage nodes will wait for wal sync of metadata log writes before sending the STORED ack.</td><td style="text-align:center">true</td><td>server only</td></tr>
</tbody>
</table>
<h2><a class="anchor" aria-hidden="true" id="write-path"></a><a href="#write-path" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Write path</h2>
<table>
<thead>
<tr><th>Name</th><th>Description</th><th style="text-align:center">Default</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>append-store-durability</td><td>The minimum guaranteed durablity of record copies before a storage node confirms the STORE as successful. Can be one of &quot;memory&quot; if record is to be stored in a RocksDB memtable only (logdeviced memory), &quot;async_write&quot; if record is to be additionally written to the RocksDB WAL file (kernel memory, frequently synced to disk), or &quot;sync_write&quot; if the record is to be written to the memtable and WAL, and the STORE acknowledged only after the WAL is synced to disk by a separate WAL syncing thread using fdatasync(3).</td><td style="text-align:center">async_write</td><td>server only</td></tr>
<tr><td>appender-buffer-process-batch</td><td>batch size for processing per-log queue of pending writes</td><td style="text-align:center">20</td><td>server only</td></tr>
<tr><td>appender-buffer-queue-cap</td><td>capacity of per-log queue of pending writes while sequencer  is initializing or activating</td><td style="text-align:center">10000</td><td>requires restart, server only</td></tr>
<tr><td>byte-offsets</td><td>Enables the server-side byte offset calculation feature.NOTE: There is no guarantee of byte offsets result correctness if featurewas switched on-&gt;off-&gt;on in period shorter than retention value forlogs.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>check-node-health-request-timeout</td><td>Timeout for health check probes that sequencers send to unresponsive storage nodes. If no reply arrives after timeout, another probe is sent.</td><td style="text-align:center">120s</td><td>server only</td></tr>
<tr><td>checksum-bits</td><td>how big a checksum to include with newly appended records (0, 32 or 64)</td><td style="text-align:center">32</td><td></td></tr>
<tr><td>copyset-locality-min-scope</td><td>Tl;dr: if you experience data distribution imbalance caused by hot logs, and you have plenty of unused cross-rack/cross-region bandwidth, try changing this setting to &quot;root&quot;; otherwise the default &quot;rack&quot; is just fine.  More details: let X be the value of this setting, and let Y be the biggest scope in log's replicateAcross property; if Y &lt; X, nothing happens; if Y &gt;= X, at least one copy of each record will be stored in sequencer's domain of scope Y (not X), when it's possible without affecting average data distribution. This, combined with chain-sending, typically reduces the number of cross-Y hops by one per record.</td><td style="text-align:center">rack</td><td>server only</td></tr>
<tr><td>disable-chain-sending</td><td>never send a wave of STORE messages through a chain</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>disable-graylisting</td><td>setting this to true disables graylisting nodes by sequencers in the write path</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>disabled-retry-interval</td><td>Time interval during which a sequencer will not route record copies to a storage node that reported a permanent error.</td><td style="text-align:center">30s</td><td>server only</td></tr>
<tr><td>enable-offset-map</td><td>Enables the server-side OffsetMap calculation feature.NOTE: There is no guarantee of byte offsets result correctness if featurewas switched on-&gt;off-&gt;on in period shorter than retention value forlogs.</td><td style="text-align:center">false</td><td>server only</td></tr>
<tr><td>epoch-metadata-use-new-storage-set-format</td><td>Serialize copysets using ShardIDs instead of node_index_t inside EpochMetaData. TODO(T15517759): enable by default once Flexible Log Sharding is fully implemented and this has been thoroughly tested.</td><td style="text-align:center">false</td><td><strong>experimental</strong></td></tr>
<tr><td>gray-list-threshold</td><td>if the number of storage nodes graylisted on the write path of a log exceeds this fraction of the log's nodeset size the gray list will be cleared to make sure that copysets can still be picked</td><td style="text-align:center">0.25</td><td>server only</td></tr>
<tr><td>isolated-sequencer-ttl</td><td>How long we wait before disabling isolated sequencers. A sequencer is declared isolated if nodes outside of the innermost failure domain of the sequencer's epoch appear unreachable to the failure detector. For example, a sequencer of a rack-replicated log epoch is declared isolated if the failure detector can't reach any nodes outside of that sequencer node's rack. A disabled sequencer rejects all append requests.</td><td style="text-align:center">1200s</td><td>server only</td></tr>
<tr><td>no-redirect-duration</td><td>when a sequencer activates upon request from a client, it does not redirect its clients to a different sequencer node for this amount of time (even if for instance the primary sequencer just started up and an older sequencer may be up and running)</td><td style="text-align:center">5s</td><td>server only</td></tr>
<tr><td>node-health-check-retry-interval</td><td>Time interval during which a node health check probe will not be sent if there is an outstanding request for the same node in the nodeset</td><td style="text-align:center">5s</td><td>server only</td></tr>
<tr><td>nodeset-state-refresh-interval</td><td>Time interval that rate-limits how often a sequencer can refresh the states of nodes in the nodeset in use</td><td style="text-align:center">1s</td><td>server only</td></tr>
<tr><td>nospace-retry-interval</td><td>Time interval during which a sequencer will not route record copies to a storage node that reported an out of disk space condition.</td><td style="text-align:center">60s</td><td>server only</td></tr>
<tr><td>overloaded-retry-interval</td><td>Time interval during which a sequencer will not route record copies to a storage node that reported itself overloaded (storage task queue too long).</td><td style="text-align:center">1s</td><td>server only</td></tr>
<tr><td>payload-inline</td><td>max message payload size that we store in a flat buffer after header</td><td style="text-align:center">1024</td><td></td></tr>
<tr><td>release-broadcast-interval</td><td>the time interval for periodic broadcasts of RELEASE messages by sequencers of regular logs. Such broadcasts are not essential for correct cluster operation. They are used as the last line of defence to make sure storage nodes deliver all records eventually even if a regular (point-to-point) RELEASE message is lost due to a TCP connection failure. See also --release-broadcast-interval-internal-logs.</td><td style="text-align:center">300s</td><td>server only</td></tr>
<tr><td>release-broadcast-interval-internal-logs</td><td>Same as --release-broadcast-interval but instead applies to internal logs, currently the event logs and logsconfig logs</td><td style="text-align:center">5s</td><td>server only</td></tr>
<tr><td>release-retry-interval</td><td>RELEASE message retry period</td><td style="text-align:center">20s</td><td>server only</td></tr>
<tr><td>slow-node-retry-interval</td><td>After a sequencer's request to store a record copy on a storage node times out that sequencer will graylist that node for this time interval. The sequencer will not pick graylisted nodes for copysets unless --gray-list-threshold is reached or no valid copyset can be selected from nodeset nodes not yet graylisted</td><td style="text-align:center">600s</td><td>server only</td></tr>
<tr><td>sticky-copysets-block-max-time</td><td>The time since starting the last block, after which the copyset manager will consider it expired and start a new one.</td><td style="text-align:center">10min</td><td>requires restart, server only</td></tr>
<tr><td>sticky-copysets-block-size</td><td>The total size of processed appends (in bytes), after which the sticky copyset manager will start a new block.</td><td style="text-align:center">33554432</td><td>requires restart, server only</td></tr>
<tr><td>store-timeout</td><td>timeout for attempts to store a record copy on a specific storage node. This value is used by sequencers only and is NOT the client request timeout.</td><td style="text-align:center">500ms..1min</td><td>server only</td></tr>
<tr><td>unroutable-retry-interval</td><td>Time interval during which a sequencer will not pick for copysets a storage node whose IP address was reported unroutable by the socket layer</td><td style="text-align:center">60s</td><td>server only</td></tr>
<tr><td>use-sequencer-affinity</td><td>If true, the routing of append requests to sequencers will first try to find a sequencer in the location given by sequencerAffinity() before looking elsewhere.</td><td style="text-align:center">false</td><td></td></tr>
<tr><td>verify-checksum-before-replicating</td><td>If set, sequencers and rebuilding will verify checksums of records that have checksums. If there is a mismatch, sequencer will reject the append. Note that this setting doesn't make storage nodes verify checksums. Note that if not set, and --rocksdb-verify-checksum-during-store is set, a corrupted record kills write-availability for that log, as the appender keeps retrying and storage nodes reject the record.</td><td style="text-align:center">true</td><td>server only</td></tr>
<tr><td>write-shard-id-in-copyset</td><td>Serialize copysets using ShardIDs instead of node_index_t on disk. TODO(T15517759): enable by default once Flexible Log Sharding is fully implemented and this has been thoroughly tested.</td><td style="text-align:center">false</td><td><strong>experimental</strong>, server only</td></tr>
<tr><td>write-sticky-copysets</td><td>If set, will enable sticky copysets and will write the copyset index for all records. This must be set before --rocksdb-use-copyset-index is enabled</td><td style="text-align:center">true</td><td>requires restart, server only</td></tr>
</tbody>
</table>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/Config.html"><span class="arrow-prev">← </span><span>Previous</span></a><a class="docs-next button" href="/docs/Logs.html"><span>Next</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav docOnPageNav"><ul class="toc-headings"><li><a href="#admin-api-server">Admin API/server</a></li><li><a href="#batching-and-compression">Batching and compression</a></li><li><a href="#configuration">Configuration</a></li><li><a href="#core-settings">Core settings</a></li><li><a href="#failure-detector">Failure detector</a></li><li><a href="#logsdb">LogsDB</a></li><li><a href="#monitoring">Monitoring</a></li><li><a href="#network-communication">Network communication</a></li><li><a href="#performance">Performance</a></li><li><a href="#read-path">Read path</a></li><li><a href="#reader-failover">Reader failover</a></li><li><a href="#rebuilding">Rebuilding</a></li><li><a href="#recovery">Recovery</a></li><li><a href="#resource-management">Resource management</a></li><li><a href="#rocksdb">RocksDB</a></li><li><a href="#security">Security</a></li><li><a href="#sequencer-state">Sequencer State</a></li><li><a href="#sequencer-boycotting">Sequencer boycotting</a></li><li><a href="#state-machine-execution">State machine execution</a></li><li><a href="#storage">Storage</a></li><li><a href="#testing">Testing</a></li><li><a href="#uncategorized">Uncategorized</a></li><li><a href="#write-path">Write path</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/logdevice.svg" alt="LogDevice" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/Overview.html">Getting Started</a><a href="/docs/FirstCluster.html">Creating your first cluster</a><a href="/api/annotated.html">C++ API Reference</a></div><div><h5>Community</h5><a href="https://facebook.com/groups/logdevice.oss/">LogDevice Users Group</a><a href="http://stackoverflow.com/questions/tagged/logdevice" target="_blank" rel="noreferrer noopener">Stack Overflow</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/facebookincubator/LogDevice">GitHub</a></div></section><a href="https://code.facebook.com/projects/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2018 Facebook</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: 'f0ece773627cb7003a57c0edd6ec7dd8',
                indexName: 'logdevice',
                inputSelector: '#search_input_react'
              });
            </script></body></html>